# PasteMax Export

## Metadata
- **Timestamp:** 2025-03-31T18:51:07.155Z
- **File Count:** 109
- **Base Folder:** `/Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning`

## Directory Structure

```
├── agentic_research
│   ├── collaborative_storm
│   │   ├── modules
│   │   │   ├── __init__.py *
│   │   │   ├── article_generation.py *
│   │   │   ├── callback.py *
│   │   │   ├── co_storm_agents.py *
│   │   │   ├── collaborative_storm_utils.py *
│   │   │   ├── costorm_expert_utterance_generator.py *
│   │   │   ├── expert_generation.py *
│   │   │   ├── grounded_question_answering.py *
│   │   │   ├── grounded_question_generation.py *
│   │   │   ├── information_insertion_module.py *
│   │   │   ├── knowledge_base_summary.py *
│   │   │   ├── simulate_user.py *
│   │   │   └── warmstart_hierarchical_chat.py *
│   │   ├── __init__.py *
│   │   └── engine.py *
│   ├── storm_analysis
│   │   ├── modules
│   │   │   ├── __init__.py *
│   │   │   ├── article_generation.py *
│   │   │   ├── article_polish.py *
│   │   │   ├── callback.py *
│   │   │   ├── knowledge_curation.py *
│   │   │   ├── outline_generation.py *
│   │   │   ├── persona_generator.py *
│   │   │   ├── retriever.py *
│   │   │   └── storm_dataclass.py *
│   │   ├── __init__.py *
│   │   └── engine.py *
│   ├── __init__.py *
│   ├── dataclass.py *
│   ├── encoder.py *
│   ├── interface.py *
│   ├── lm.py *
│   ├── logging_wrapper.py *
│   ├── rm.py *
│   └── utils.py *
├── scripts
│   ├── agentic_reason
│   │   ├── __init__.py *
│   │   ├── cache.py *
│   │   ├── config.py *
│   │   ├── data_loader.py *
│   │   ├── generation.py *
│   │   ├── models.py *
│   │   ├── prompt_manager.py *
│   │   ├── search.py *
│   │   └── utils.py *
│   ├── agentic_research
│   │   ├── __init__.py *
│   │   └── rm.py *
│   ├── lcb_runner
│   │   ├── benchmarks
│   │   │   ├── __init__.py *
│   │   │   ├── code_execution.py *
│   │   │   └── code_generation.py *
│   │   ├── evaluation
│   │   │   ├── __init__.py *
│   │   │   ├── compute_code_execution_metrics.py *
│   │   │   ├── compute_code_generation_metrics.py *
│   │   │   ├── compute_scores.py *
│   │   │   ├── compute_test_output_prediction_metrics.py *
│   │   │   ├── old_results_check.py *
│   │   │   ├── pass_k_utils.py *
│   │   │   ├── testing_util.py *
│   │   │   └── utils_execute.py *
│   │   ├── prompts
│   │   │   ├── few_shot_examples
│   │   │   │   └── generation
│   │   │   │       ├── func.json *
│   │   │   │       └── stdin.json *
│   │   │   ├── __init__.py *
│   │   │   ├── code_execution.py *
│   │   │   ├── code_generation.py *
│   │   │   └── self_repair.py *
│   │   ├── pyext
│   │   │   └── pyext-0.7
│   │   │       ├── PKG-INFO *
│   │   │       ├── pyext.py *
│   │   │       ├── README.rst *
│   │   │       ├── setup.cfg *
│   │   │       └── setup.py *
│   │   ├── runner
│   │   │   ├── base_runner.py *
│   │   │   ├── claude_runner.py *
│   │   │   ├── claude3_runner.py *
│   │   │   ├── cohere_runner.py *
│   │   │   ├── custom_evaluator.py *
│   │   │   ├── deepseek_runner.py *
│   │   │   ├── gemini_runner.py *
│   │   │   ├── main.py *
│   │   │   ├── mistral_runner.py *
│   │   │   ├── oai_runner.py *
│   │   │   ├── parser.py *
│   │   │   ├── runner_utils.py *
│   │   │   ├── scenario_router.py *
│   │   │   └── vllm_runner.py *
│   │   ├── utils
│   │   │   ├── extraction_utils.py *
│   │   │   ├── multiprocess.py *
│   │   │   ├── path_utils.py *
│   │   │   └── scenarios.py *
│   │   └── lm_styles.py *
│   ├── tools
│   │   ├── __init__.py *
│   │   ├── bing_search.py *
│   │   ├── creat_graph.py *
│   │   ├── duck_search.py *
│   │   ├── run_code.py *
│   │   ├── run_search.py *
│   │   └── temp.py *
│   ├── utils
│   │   ├── math_equivalence.py *
│   │   └── remote_llm.py *
│   ├── __init__.py *
│   ├── agentic_ds.py *
│   ├── evaluate.py *
│   ├── github_upload.py *
│   ├── prompts.py *
│   ├── run_agentic_reason.py *
│   └── write_paper.py *
├── .gitignore *
├── Agentic Reasoning environment.yml *
├── pyproject.toml *
├── README.md *
├── setup.py *
└── temp.py *

```

## Files

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_research/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/benchmarks/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/prompts/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/__init__.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/.gitignore
```gitignore
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/Agentic Reasoning environment.yml
```yml
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_ds.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/article_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/article_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/article_polish.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/base_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/bing_search.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/cache.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/callback.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/callback.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/claude_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/claude3_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/co_storm_agents.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/benchmarks/code_execution.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/prompts/code_execution.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/benchmarks/code_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/prompts/code_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/cohere_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/collaborative_storm_utils.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/compute_code_execution_metrics.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/compute_code_generation_metrics.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/compute_scores.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/compute_test_output_prediction_metrics.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/config.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/costorm_expert_utterance_generator.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/creat_graph.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/custom_evaluator.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/data_loader.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/dataclass.py
```py
import dspy
import numpy as np
import re
import threading
from typing import Set, Dict, List, Optional, Union, Tuple

from .encoder import Encoder
from .interface import Information


class ConversationTurn:
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/deepseek_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/duck_search.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/encoder.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/engine.py
```py
import dspy
import os
from dataclasses import dataclass, field, asdict
from typing import List, Union, Literal, Optional, Dict

from .modules import collaborative_storm_utils as collaborative_storm_utils
from .modules.callback import BaseCallbackHandler
from .modules.co_storm_agents import (
    SimulatedUser,
    PureRAGAgent,
    Moderator,
    CoStormExpert,
)
from .modules.expert_generation import GenerateExpertModule
from .modules.warmstart_hierarchical_chat import WarmStartModule
from ..dataclass import ConversationTurn, KnowledgeBase
from ..encoder import Encoder
from ..interface import LMConfigs, Agent
from ..logging_wrapper import LoggingWrapper
from ..lm import LitellmModel
from ..rm import BingSearch


class CollaborativeStormLMConfigs(LMConfigs):
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/engine.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/evaluate.py
```py
import re
import json
import numpy as np
from collections import Counter
import string
import os, time
from collections import defaultdict
from lcb_runner.evaluation import codegen_metrics
from utils.math_equivalence import is_equiv


def extract_answer(output, mode='gen'):
    /* ... body removed by PasteMax ... */


def normalize_answer(text):
    /* ... body removed by PasteMax ... */

def normalize_answer_qa(s):
    /* ... body removed by PasteMax ... */_answer, mode='gen'):
    /* ... body removed by PasteMax ... */



def run_evaluation(filtered_data, input_list, output_list, dataset_name, output_dir, total_time, split, apply_backoff=False):
    /* ... body removed by PasteMax ... */



if __name__ == "__main__":
    import argparse

    # Parse command-line arguments for flexibility
    parser = argparse.ArgumentParser(description="Evaluate model outputs with optional backoff.")
    parser.add_argument('--output_path', type=str, required=True, help='Path to the model output JSON file.')
    parser.add_argument('--output_metrics_path', type=str, help='Path to save the evaluation metrics.')
    parser.add_argument('--apply_backoff', action='store_true', help='Enable backoff to normal outputs if main output is invalid.')
    args = parser.parse_args()

    output_path = args.output_path
    if args.output_metrics_path:
        output_metrics_path = args.output_metrics_path
    else:
        output_metrics_path = output_path.replace('.json', '.metrics.json')

    # Determine dataset name based on the output path
    # NOTE: To apply back off strategy for retrieval-augmented reasoning methods, please replace normal_output_path with your actual path for results with run_direct_gen.
    if 'gpqa' in output_path:
        dataset_name = 'gpqa'
        normal_output_path = './outputs/gpqa.qwq.direct/diamond.12.13,18:23.json'
        if 'extended' in output_path:
            normal_output_path = './outputs/gpqa.qwq.direct/extended.12.28,15:44.json'
        if 'qwq' not in output_path:
            normal_output_path = './outputs/runs.baselines/gpqa.qwen2.5-32b-instruct.direct/diamond.12.14,20:34.json'
    elif 'math500' in output_path:
        dataset_name = 'math500'
        normal_output_path = './outputs/math500.qwq.direct/test.12.13,18:26.json'
        if 'qwq' not in output_path:
            normal_output_path = './outputs/runs.baselines/math500.qwen2.5-32b-instruct.direct/test.12.15,10:43.json'
    elif 'aime' in output_path:
        dataset_name = 'aime'
        normal_output_path = './outputs/aime.qwq.direct/2024.12.13,19:36.json'
        if 'qwq' not in output_path:
            normal_output_path = './outputs/runs.baselines/aime.qwen2.5-32b-instruct.direct/test.12.14,20:28.json'
    elif 'amc' in output_path:
        dataset_name = 'amc'
        normal_output_path = './outputs/amc.qwq.direct/test.12.14,14:31.json'
        if 'qwq' not in output_path:
            normal_output_path = './outputs/runs.baselines/amc.qwen2.5-32b-instruct.direct/test.12.14,20:26.json'
    elif 'livecode' in output_path:
        dataset_name = 'livecode'
        normal_output_path = './outputs/livecode.qwq.direct/test.12.13,21:24.json'
        if 'qwq' not in output_path:
            normal_output_path = './outputs/runs.baselines/livecode.qwen2.5-32b-instruct.direct/test.12.14,20:32.json'
    elif 'nq' in output_path:
        dataset_name = 'nq'
        normal_output_path = './outputs/runs.qa/nq.qwq.direct/test.12.15,14:50.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif 'triviaqa' in output_path:
        dataset_name = 'triviaqa'
        normal_output_path = './outputs/runs.qa/triviaqa.qwq.direct/test.12.15,15:35.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif 'hotpotqa' in output_path:
        dataset_name = 'hotpotqa'
        normal_output_path = './outputs/runs.qa/hotpotqa.qwq.direct/test.12.15,14:52.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif 'musique' in output_path:
        dataset_name = 'musique'
        normal_output_path = './outputs/runs.qa/musique.qwq.direct/test.12.27,16:44.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif 'bamboogle' in output_path:
        dataset_name = 'bamboogle'
        normal_output_path = './outputs/runs.qa/bamboogle.qwq.direct/test.12.28,9:51.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif '2wiki' in output_path:
        dataset_name = '2wiki'
        normal_output_path = './outputs/runs.qa/2wiki.qwq.direct/test.12.15,15:32.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif 'medmcqa' in output_path:
        dataset_name = 'medmcqa'
        normal_output_path = './outputs/runs.qa/medmcqa.qwq.direct/test.12.15,16:57.json'
        if 'qwq' not in output_path:
            normal_output_path = ''
    elif 'pubhealth' in output_path:
        dataset_name = 'pubhealth'
        normal_output_path = './outputs/runs.qa/pubhealth.qwq.direct/test.12.15,20:32.json'
        if 'qwq' not in output_path:
            normal_output_path = ''

    # Load main output data
    with open(output_path, mode='r', encoding='utf-8') as file:
        data = json.load(file)

    # Load main metrics data
    with open(output_metrics_path, mode='r', encoding='utf-8') as file:
        metrics = json.load(file)

    # Extract existing metrics
    if 'overall' in metrics:
        query_latency = metrics['overall']['query_latency']
        original_num_valid_answer = metrics['overall']['num_valid_answer']
    else:
        query_latency = metrics.get('query_latency', 'N/A')
        original_num_valid_answer = metrics.get('num_valid_answer', 'N/A')

    # Load normal output data if backoff is enabled
    normal_data = None
    if args.apply_backoff:
        if not os.path.exists(normal_output_path):
            raise FileNotFoundError(f"Normal output file not found at: {normal_output_path}")
        with open(normal_output_path, mode='r', encoding='utf-8') as file:
            normal_data = json.load(file)

    if dataset_name != 'livecode':
        # Existing evaluation for non-livecode datasets
        avg_em, avg_acc, avg_f1, avg_math = [], [], [], []
        num_valid_answer = 0

        # Initialize per-domain metrics
        domain_metrics = {}

        for i, item in enumerate(data):
            if dataset_name in ['gpqa', 'medmcqa']:
                labeled_answer = item["Correct Choice"]
                domain = item.get("High-level domain", "Unknown")
                mode = 'choose'
            elif dataset_name == 'math500':
                labeled_answer = item["answer"]
                domain = item.get("level", "Unknown")
                mode = 'gen'
            elif dataset_name in ['aime', 'amc']:
                labeled_answer = item["answer"]
                mode = 'gen'
                domain = 'Unknown'
            elif dataset_name in ['nq', 'triviaqa', 'hotpotqa', 'musique', 'bamboogle', '2wiki']:
                labeled_answer = item["answer"]
                mode = 'qa'
                domain = 'Unknown'
            elif dataset_name in ['pubhealth']:
                labeled_answer = item["answer"]
                mode = 'choose'
                domain = 'Unknown'
            else:
                raise ValueError(f"Unsupported dataset: {dataset_name}")

            output = item['Output']

            metric, pred_answer = evaluate_predictions(
                output=output, 
                labeled_answer=labeled_answer,
                mode=mode,
            )

            # Determine if the main method's answer is valid
            my_method_valid = (pred_answer != '' and not (mode == 'choose' and dataset_name == 'gpqa' and len(pred_answer) > 1))

            # If invalid and backoff is enabled, use normal method's output
            if args.apply_backoff and not my_method_valid and normal_data is not None:
                normal_item = normal_data[i]
                if dataset_name in ['gpqa', 'medmcqa']:
                    normal_labeled_answer = normal_item["Correct Choice"]
                    normal_mode = 'choose'
                elif dataset_name == 'math500':
                    normal_labeled_answer = normal_item["answer"]
                    normal_mode = 'gen'
                elif dataset_name in ['aime', 'amc']:
                    normal_labeled_answer = normal_item["answer"]
                    normal_mode = 'gen'
                elif dataset_name in ['nq', 'triviaqa', 'hotpotqa', 'musique', 'bamboogle', '2wiki']:
                    normal_labeled_answer = normal_item["answer"]
                    normal_mode = 'qa'
                elif dataset_name in ['pubhealth']:
                    normal_labeled_answer = normal_item["answer"]
                    normal_mode = 'choose'
                else:
                    raise ValueError(f"Unsupported dataset for backoff: {dataset_name}")

                normal_output = normal_item['Output']

                normal_metric, normal_pred_answer = evaluate_predictions(
                    output=normal_output, 
                    labeled_answer=normal_labeled_answer,
                    mode=normal_mode,
                )
                normal_valid = (normal_pred_answer != '' and not (normal_mode == 'choose' and dataset_name == 'gpqa' and len(normal_pred_answer) > 1))

                # Use normal method's result if valid
                if normal_valid:
                    metric = normal_metric
                    pred_answer = normal_pred_answer
                    my_method_valid = True

            # Track metrics per domain
            if domain not in domain_metrics:
                domain_metrics[domain] = {'em': [], 'acc': [], 'f1': [], 'math_equal': [], 'num_valid_answer': 0, 'total_num': 0}
            domain_metrics[domain]['total_num'] += 1
                
            avg_em.append(metric['em'])
            avg_acc.append(metric['acc'])
            avg_f1.append(metric['f1'])
            avg_math.append(metric['math_equal'])
            domain_metrics[domain]['em'].append(metric['em'])
            domain_metrics[domain]['acc'].append(metric['acc'])
            domain_metrics[domain]['f1'].append(metric['f1'])
            domain_metrics[domain]['math_equal'].append(metric['math_equal'])

            if my_method_valid:
                num_valid_answer += 1
                domain_metrics[domain]['num_valid_answer'] += 1

        # Compute overall metrics
        overall_metrics = {
            'em': np.mean(avg_em) if len(avg_em) > 0 else 0, 
            'acc': np.mean(avg_acc) if len(avg_acc) > 0 else 0, 
            'f1': np.mean(avg_f1) if len(avg_f1) > 0 else 0, 
            'math_equal': np.mean(avg_math) if len(avg_math) > 0 else 0, 
            'num_valid_answer': f'{num_valid_answer} of {len(data)}',
            'query_latency': query_latency,
        }
        if args.apply_backoff:
            overall_metrics['original_num_valid_answer'] = original_num_valid_answer

        # Compute per-domain metrics
        domain_avg_metrics = {}
        for dm, m in domain_metrics.items():
            domain_avg_metrics[dm] = {
                'em': np.mean(m['em']) if len(m['em']) > 0 else 0,
                'acc': np.mean(m['acc']) if len(m['acc']) > 0 else 0,
                'f1': np.mean(m['f1']) if len(m['f1']) > 0 else 0,
                'math_equal': np.mean(m['math_equal']) if len(m['math_equal']) > 0 else 0,
                'num_valid_answer': f'{m["num_valid_answer"]} of {m["total_num"]}',
            }

        # Prepare final metrics
        final_metrics = {'overall': overall_metrics}
        if dataset_name == 'gpqa':
            final_metrics['per_domain'] = domain_avg_metrics

    else:
        # Evaluation and backoff for livecode dataset
        split = 'test'  # Modify as needed or extract from output_path

        if args.apply_backoff and normal_data is not None:
            # Apply backoff by replacing invalid outputs with normal outputs
            for i, item in enumerate(data):
                # Extract Pred_Answer from main output
                pred_answer = item['Pred_Answer']

                # Check if Pred_Answer is invalid
                if pred_answer == '':
                    # Replace Output with normal output
                    item['Output'] = normal_data[i]['Output']

        # Prepare input_list and output_list for run_evaluation
        input_list = [item['Question'] for item in data]
        output_list = [item['Output'] for item in data]

        # Estimate total_time (if available). Here, set to 0 as a placeholder.
        total_time = 0  # Modify if timing information is available

        # Run evaluation
        run_evaluation(
            filtered_data=data,
            input_list=input_list,
            output_list=output_list,
            dataset_name=dataset_name,
            output_dir=output_path,
            total_time=total_time,
            split=split,
            apply_backoff=True,
        )
        # run_evaluation handles saving the metrics for livecode

    # Save metrics for non-livecode datasets
    if dataset_name != 'livecode' or not args.apply_backoff:
        # If dataset is livecode and backoff was applied, metrics are already saved by run_evaluation
        if args.apply_backoff:
            output_metrics_path = output_metrics_path.replace('.json', '.backoff.json')
        with open(output_metrics_path, mode='w', encoding='utf-8') as json_file:
            json.dump(final_metrics, json_file, indent=4, ensure_ascii=False)

    print(f"Evaluation completed. Metrics saved to {output_metrics_path}")

```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/expert_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/utils/extraction_utils.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/prompts/few_shot_examples/generation/func.json
```json
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/gemini_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/github_upload.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/grounded_question_answering.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/grounded_question_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/information_insertion_module.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/interface.py
```py
import concurrent.futures
import dspy
import functools
import hashlib
import json
import logging
import time
from abc import ABC, abstractmethod
from collections import OrderedDict
from typing import Dict, List, Optional, Union, TYPE_CHECKING

from .utils import ArticleTextProcessing

logging.basicConfig(
    level=logging.INFO, format="%(name)s : %(levelname)-8s : %(message)s"
)
logger = logging.getLogger(__name__)

if TYPE_CHECKING:
    from .logging_wrapper import LoggingWrapper


class InformationTable(ABC):
    /* ... body removed by PasteMax ... */nformation(**kwargs):
        /* ... body removed by PasteMax ... */


class Information:
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/knowledge_base_summary.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/knowledge_curation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/lm_styles.py
```py
from dataclasses import dataclass
from datetime import datetime
from enum import Enum


class LMStyle(Enum):
    /* ... body removed by PasteMax ... */


@dataclass
class LanguageModel:
    /* ... body removed by PasteMax ... */Max ... */


LanguageModelList: list[LanguageModel] = [
    LanguageModel(
        "meta-llama/Meta-Llama-3-70B",
        "LLama3-70b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3-70B",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3-8B",
        "LLama3-8b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3-8B",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3-8B-Instruct",
        "LLama3-8b-Ins",
        LMStyle.LLaMa3,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3-70B-Instruct",
        "LLama3-70b-Ins",
        LMStyle.LLaMa3,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3.1-8B",
        "LLama3.1-8b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3.1-70B",
        "LLama3.1-70b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3.1-405B-FP8",
        "LLama3.1-405b-Base-FP8",
        LMStyle.LLaMa3,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "LLama3.1-8b-Ins",
        LMStyle.LLaMa3,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3.1-70B-Instruct",
        "LLama3.1-70b-Ins",
        LMStyle.LLaMa3,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
    ),
    LanguageModel(
        "meta-llama/Meta-Llama-3.1-405B-Instruct-FP8",
        "LLama3.1-405b-Ins-FP8",
        LMStyle.LLaMa3,
        datetime(2023, 1, 1),
        link="https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct-FP8",
    ),
    LanguageModel(
        "deepseek-ai/deepseek-coder-33b-base",
        "DSCoder-33b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/deepseek-ai/deepseek-coder-33b-base",
    ),
    LanguageModel(
        "deepseek-ai/deepseek-coder-6.7b-base",
        "DSCoder-6.7b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-base",
    ),
    LanguageModel(
        "deepseek-ai/deepseek-coder-1.3b-base",
        "DSCoder-1.3b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-base",
    ),
    LanguageModel(
        "deepseek-ai/deepseek-coder-33b-instruct",
        "DSCoder-33b-Ins",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 9, 1),
        link="https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct",
    ),
    LanguageModel(
        "deepseek-ai/deepseek-coder-6.7b-instruct",
        "DSCoder-6.7b-Ins",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 9, 1),
        link="https://huggingface.co/deepseek-ai/deepseek-coder-6.7b-instruct",
    ),
    LanguageModel(
        "deepseek-ai/deepseek-coder-1.3b-instruct",
        "DSCoder-1.3b-Ins",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 8, 1),
        link="https://huggingface.co/deepseek-ai/deepseek-coder-1.3b-instruct",
    ),
    LanguageModel(
        "deepseek-chat",
        "DeepSeek-V2",
        LMStyle.DeepSeekAPI,
        datetime(2023, 8, 1),
        link="https://huggingface.co/deepseek-ai/DeepSeek-V2",
    ),
    LanguageModel(
        "deepseek-coder",
        "DeepSeekCoder-V2.5",
        LMStyle.DeepSeekAPI,
        datetime(2023, 8, 1),
        link="https://huggingface.co/deepseek-ai/DeepSeek-V2",
    ),
    LanguageModel(
        "deepseek-ai/DeepSeek-V2-Chat",
        "DeepSeek-V2-Chat",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 12, 30),
        link="https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat",
    ),
    LanguageModel(
        "deepseek-ai/DeepSeek-Coder-V2-Instruct",
        "DeepSeek-Coder-V2-Instruct",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 12, 30),
        link="https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct",
    ),
    LanguageModel(
        "deepseek-ai/DeepSeek-V2-Chat-0628",
        "DeepSeek-V2-Chat-0628",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 12, 30),
        link="https://huggingface.co/deepseek-ai/DeepSeek-V2-Chat-0628",
    ),
    LanguageModel(
        "deepseek-ai/DeepSeek-Coder-V2-Instruct-0724",
        "DeepSeek-Coder-V2-Instruct-0724",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 12, 30),
        link="https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Instruct-0724",
    ),
    LanguageModel(
        "deepseek-ai/DeepSeek-V2-Lite-Chat",
        "DeepSeek-V2-Lite-Chat",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 12, 30),
        link="https://huggingface.co/deepseek-ai/DeepSeek-V2-Lite-Chat",
    ),
    LanguageModel(
        "deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
        "DeepSeek-Coder-V2-Lite-Instruct",
        LMStyle.DeepSeekCodeInstruct,
        datetime(2023, 12, 30),
        link="https://huggingface.co/deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct",
    ),
    LanguageModel(
        "codellama/CodeLlama-70b-hf",
        "CodeLlama-70b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-70b-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-34b-hf",
        "CodeLlama-34b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-34b-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-13b-hf",
        "CodeLlama-13b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-13b-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-7b-hf",
        "CodeLlama-7b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-7b-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-70b-Instruct-hf",
        "CodeLlama-70b-Ins",
        LMStyle.CodeLLaMaInstruct,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-70b-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-34b-Instruct-hf",
        "CodeLlama-34b-Ins",
        LMStyle.CodeLLaMaInstruct,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-13b-Instruct-hf",
        "CodeLlama-13b-Ins",
        LMStyle.CodeLLaMaInstruct,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-13b-Instruct-hf",
    ),
    LanguageModel(
        "codellama/CodeLlama-7b-Instruct-hf",
        "CodeLlama-7b-Ins",
        LMStyle.CodeLLaMaInstruct,
        datetime(2023, 1, 1),
        link="https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf",
    ),
    LanguageModel(
        "gpt-3.5-turbo-0301",
        "GPT-3.5-Turbo-0301",
        LMStyle.OpenAIChat,
        datetime(2021, 10, 1),
        link="https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
    ),
    LanguageModel(
        "gpt-3.5-turbo-0125",
        "GPT-3.5-Turbo-0125",
        LMStyle.OpenAIChat,
        datetime(2021, 10, 1),
        link="https://openai.com/blog/new-embedding-models-and-api-updates#:~:text=Other%20new%20models%20and%20lower%20pricing",
    ),
    LanguageModel(
        "gpt-4-0613",
        "GPT-4-0613",
        LMStyle.OpenAIChat,
        datetime(2021, 10, 1),
        link="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",
    ),
    LanguageModel(
        "gpt-4-1106-preview",
        "GPT-4-Turbo-1106",
        LMStyle.OpenAIChat,
        datetime(2023, 4, 30),
        link="https://openai.com/blog/new-models-and-developer-products-announced-at-devday",
    ),
    LanguageModel(
        "gpt-4-turbo-2024-04-09",
        "GPT-4-Turbo-2024-04-09",
        LMStyle.OpenAIChat,
        datetime(2023, 4, 30),
        link="https://platform.openai.com/docs/models/gpt-4-turbo-and-gpt-4",
    ),
    LanguageModel(
        "gpt-4o-2024-05-13",
        "GPT-4O-2024-05-13",
        LMStyle.OpenAIChat,
        datetime(2023, 4, 30),
        link="https://openai.com/index/spring-update",
    ),
    LanguageModel(
        "gpt-4o-2024-08-06",
        "GPT-4O-2024-08-06",
        LMStyle.OpenAIChat,
        datetime(2023, 4, 30),
        link="https://openai.com/index/spring-update",
    ),
    LanguageModel(
        "gpt-4o-mini-2024-07-18",
        "GPT-4O-mini-2024-07-18",
        LMStyle.OpenAIChat,
        datetime(2023, 4, 30),
        link="https://openai.com/index/spring-update",
    ),
    LanguageModel(
        "chatgpt-4o-latest",
        "ChatGPT-4O-Latest-08-24",
        LMStyle.OpenAIChat,
        datetime(2023, 4, 30),
        link="https://openai.com/index/spring-update",
    ),
    LanguageModel(
        "o1-preview-2024-09-12",
        "O1-Preview-2024-09-12 (N=1)",
        LMStyle.OpenAIReason,
        datetime(2023, 4, 30),
        link="https://openai.com/index/spring-update",
    ),
    LanguageModel(
        "o1-mini-2024-09-12",
        "O1-Mini-2024-09-12 (N=1)",
        LMStyle.OpenAIReason,
        datetime(2023, 4, 30),
        link="https://openai.com/index/spring-update",
    ),
    LanguageModel(
        "claude-instant-1",
        "Claude-Instant-1",
        LMStyle.Claude,
        datetime(2022, 12, 31),
        link="https://www.anthropic.com/index/introducing-claude",
    ),
    LanguageModel(
        "claude-2",
        "Claude-2",
        LMStyle.Claude,
        datetime(2022, 12, 31),
        link="https://www.anthropic.com/index/claude-2",
    ),
    LanguageModel(
        "claude-3-opus-20240229",
        "Claude-3-Opus",
        LMStyle.Claude3,
        datetime(2023, 9, 1),
        link="https://www.anthropic.com/index/claude-3",
    ),
    LanguageModel(
        "claude-3-sonnet-20240229",
        "Claude-3-Sonnet",
        LMStyle.Claude3,
        datetime(2023, 9, 1),
        link="https://www.anthropic.com/index/claude-3",
    ),
    LanguageModel(
        "claude-3-5-sonnet-20240620",
        "Claude-3.5-Sonnet",
        LMStyle.Claude3,
        datetime(2024, 3, 31),
        link="https://www.anthropic.com/news/claude-3-5-sonnet",
    ),
    LanguageModel(
        "claude-3-haiku-20240307",
        "Claude-3-Haiku",
        LMStyle.Claude3,
        datetime(2023, 4, 30),
        link="https://www.anthropic.com/index/claude-3",
    ),
    LanguageModel(
        "gemini-1.5-pro-002",
        "Gemini-Pro-1.5-002",
        LMStyle.Gemini,
        datetime(2023, 4, 30),
        link="https://blog.google/technology/ai/gemini-api-developers-cloud",
    ),
    LanguageModel(
        "gemini-1.5-flash-002",
        "Gemini-Flash-1.5-002",
        LMStyle.Gemini,
        datetime(2023, 4, 30),
        link="https://blog.google/technology/ai/gemini-api-developers-cloud",
    ),
    LanguageModel(
        "databricks-dbrx-instruct",
        "DBRX-Ins",
        LMStyle.DataBricks,
        datetime(2023, 1, 1),
        link="https://huggingface.co/databricks/dbrx-instruct",
    ),
    LanguageModel(
        "bigcode/starcoder2-3b",
        "StarCoder2-3b",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/bigcode/starcoder2-7b-magicoder-instruct/tree/main",
    ),
    LanguageModel(
        "bigcode/starcoder2-7b",
        "StarCoder2-7b",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/bigcode/starcoder2-7b-magicoder-instruct/tree/main",
    ),
    LanguageModel(
        "bigcode/starcoder2-15b",
        "StarCoder2-15b",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/bigcode/starcoder2-7b-magicoder-instruct/tree/main",
    ),
    LanguageModel(
        "google/codegemma-7b",
        "CodeGemma-7b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/google/codegemma-7b",
    ),
    LanguageModel(
        "google/codegemma-2b",
        "CodeGemma-2b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/google/codegemma-2b",
    ),
    LanguageModel(
        "google/gemma-7b",
        "Gemma-7b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/google/gemma-7b",
    ),
    LanguageModel(
        "google/gemma-2b",
        "Gemma-2b-Base",
        LMStyle.GenericBase,
        datetime(2023, 1, 1),
        link="https://huggingface.co/google/gemma-2b",
    ),
    LanguageModel(
        "mistral-large-latest",
        "Mistral-Large",
        LMStyle.MistralWeb,
        datetime(2023, 1, 1),
        link="https://mistral.ai/news/mistral-large/",
    ),
    LanguageModel(
        "open-mixtral-8x22b",
        "Mixtral-8x22B-Ins",
        LMStyle.MistralWeb,
        datetime(2023, 1, 1),
        link="https://mistral.ai/news/mixtral-8x22b/",
    ),
    LanguageModel(
        "open-mixtral-8x7b",
        "Mixtral-8x7B-Ins",
        LMStyle.MistralWeb,
        datetime(2023, 1, 1),
        link="https://mistral.ai/news/mixtral-8x7b/",
    ),
    LanguageModel(
        "open-mixtral-8x7b",
        "Mixtral-8x7B-Ins",
        LMStyle.MistralWeb,
        datetime(2023, 1, 1),
        link="https://mistral.ai/news/mixtral-8x7b/",
    ),
    LanguageModel(
        "codestral-latest",
        "Codestral-Latest",
        LMStyle.MistralWeb,
        datetime(2023, 1, 1),
        link="https://mistral.ai/news/codestral/",
    ),
    LanguageModel(
        "command-r",
        "Command-R",
        LMStyle.CohereCommand,
        datetime(2023, 1, 1),
        link="https://docs.cohere.com/docs/models",
    ),
    LanguageModel(
        "command-r-plus",
        "Command-R+",
        LMStyle.CohereCommand,
        datetime(2023, 1, 1),
        link="https://docs.cohere.com/docs/models",
    ),
    LanguageModel(
        "Qwen/CodeQwen1.5-7B",
        "CodeQwen15-7B",
        LMStyle.GenericBase,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/CodeQwen1.5-7B",
    ),
    LanguageModel(
        "Qwen/CodeQwen1.5-7B-Chat",
        "CodeQwen15-7B-Chat",
        LMStyle.CodeQwenInstruct,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/CodeQwen1.5-7B-Chat",
    ),
    LanguageModel(
        "Qwen/Qwen2-72B",
        "Qwen2-Base-72B",
        LMStyle.GenericBase,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2-72B",
    ),
    LanguageModel(
        "Qwen/Qwen2-72B-Instruct",
        "Qwen2-Ins-72B",
        LMStyle.CodeQwenInstruct,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2-72B-Instruct",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-7B",
        "Qwen2.5-Base-7B",
        LMStyle.GenericBase,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-7B",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-7B-Instruct",
        "Qwen2.5-Ins-7B",
        LMStyle.CodeQwenInstruct,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-32B",
        "Qwen2.5-Base-32B",
        LMStyle.GenericBase,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-32B",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-32B-Instruct",
        "Qwen2.5-Ins-32B",
        LMStyle.CodeQwenInstruct,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-32B-Instruct",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-72B",
        "Qwen2.5-Base-72B",
        LMStyle.GenericBase,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-72B",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-72B-Instruct",
        "Qwen2.5-Ins-72B",
        LMStyle.CodeQwenInstruct,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-Coder-7B",
        "Qwen2.5-Coder-Base-7B",
        LMStyle.GenericBase,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-Coder-7B",
    ),
    LanguageModel(
        "Qwen/Qwen2.5-Coder-7B-Instruct",
        "Qwen2.5-Coder-Ins-7B",
        LMStyle.CodeQwenInstruct,
        datetime(2023, 8, 30),
        link="https://huggingface.co/Qwen/Qwen2.5-Coder-7B-Instruct",
    ),
    LanguageModel(
        "m-a-p/OpenCodeInterpreter-DS-33B",
        "OC-DS-33B",
        LMStyle.OC,
        datetime(2023, 1, 1),
        link="https://huggingface.co/m-a-p/OpenCodeInterpreter-DS-33B/",
    ),
    LanguageModel(
        "m-a-p/OpenCodeInterpreter-DS-6.7B",
        "OC-DS-6.7B",
        LMStyle.OC,
        datetime(2023, 9, 1),
        link="https://huggingface.co/m-a-p/OpenCodeInterpreter-DS-6.7B/",
    ),
    LanguageModel(
        "m-a-p/OpenCodeInterpreter-DS-1.3B",
        "OC-DS-1.3B",
        LMStyle.OC,
        datetime(2023, 9, 1),
        link="https://huggingface.co/m-a-p/OpenCodeInterpreter-DS-1.3B/",
    ),
    LanguageModel(
        "stabilityai/stable-code-3b",
        "StableCode-3B",
        LMStyle.GenericBase,
        datetime(2023, 9, 1),
        link="https://huggingface.co/stabilityai/stable-code-3b/",
    ),
    LanguageModel(
        "bigcode/starcoder2-instruct-15b-v0.1",
        "StarCoder2-Ins-v0.1",
        LMStyle.LLaMa3,
        datetime(2023, 4, 30),
        link="https://huggingface.co/bigcode/starcoder2-instruct-15b-v0.1",
    ),
    LanguageModel(
        "qwen/Qwen1.5-72B-Chat",
        "Qwen-1.5-72B-Chat ",
        LMStyle.Qwen1point5,
        datetime(2024, 3, 31),
        link="https://huggingface.co/qwen/Qwen1.5-72B-Chat/",
    ),
    LanguageModel(
        "abacusai/Smaug-2-72B",
        "Smaug-2-72B ",
        LMStyle.Smaug2,
        datetime(2024, 3, 31),
        link="https://huggingface.co/abacusai/Smaug-2-72B/",
    ),
    LanguageModel(
        "WizardCoderLM/WizardCoderCoder-Python-34B-V1.0",
        "WCoder-34B-V1",
        LMStyle.WizardCoder,
        datetime(2023, 1, 1),
        link="https://huggingface.co/WizardCoderLM/WizardCoderCoder-Python-34B-V1.0",
    ),
    LanguageModel(
        "WizardCoderLM/WizardCoderCoder-33B-V1.1",
        "WCoder-33B-V1.1",
        LMStyle.WizardCoder,
        datetime(2023, 9, 1),
        link="https://huggingface.co/WizardCoderLM/WizardCoderCoder-33B-V1.1",
    ),
    LanguageModel(
        "Phind/Phind-CodeLlama-34B-v2",
        "Phind-34B-V2",
        LMStyle.Phind,
        datetime(2023, 1, 1),
        link="https://huggingface.co/Phind/Phind-CodeLlama-34B-v2",
    ),
    LanguageModel(
        "ise-uiuc/Magicoder-S-DS-6.7B",
        "MagiCoderS-DS-6.7B",
        LMStyle.MagiCoder,
        datetime(2023, 7, 30),
        link="https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B",
    ),
    LanguageModel(
        "ise-uiuc/Magicoder-S-CL-7B",
        "MagiCoderS-CL-7B",
        LMStyle.MagiCoder,
        datetime(2023, 1, 1),
        link="https://huggingface.co/ise-uiuc/Magicoder-S-CL-7B",
    ),
    LanguageModel(
        "openbmb/Eurus-70b-sft",
        "Eurus-70B-SFT (n=1)",
        LMStyle.Eurusx,
        datetime(2023, 1, 1),
        link="https://huggingface.co/openbmb/Eurus-70b-sft",
    ),
    LanguageModel(
        "openbmb/Eurux-8x22b-nca",
        "Eurux-8x22b-NCA (n=1)",
        LMStyle.Eurusx,
        datetime(2023, 4, 30),
        link="https://huggingface.co/openbmb/Eurux-8x22b-nca",
    ),
    LanguageModel(
        "abacusai/Dracarys-Llama-3.1-70B-Instruct",
        "LLama3.1-70b-Ins",
        LMStyle.DracarysLlama,
        datetime(2023, 1, 1),
        link="https://huggingface.co/abacusai/Dracarys-Llama-3.1-70B-Instruct",
    ),
    LanguageModel(
        "abacusai/Dracarys-72B-Instruct",
        "Qwen2-Ins-72B",
        LMStyle.DracarysQwen,
        datetime(2023, 1, 1),
        link="https://huggingface.co/abacusai/Dracarys-72B-Instruct",
    ),
]

LanguageModelStore: dict[str, LanguageModel] = {
    lm.model_name: lm for lm in LanguageModelList
}

if __name__ == "__main__":
    print(list(LanguageModelStore.keys()))

```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/lm.py
```py
import backoff
import dspy
import functools
import logging
import os
import random
import requests
import threading
from typing import Optional, Literal, Any
import ujson
from pathlib import Path


# from dsp import ERRORS, backoff_hdlr, giveup_hdlr
# from dsp.modules.hf import openai_to_hf
# from dsp.modules.hf_client import send_hftgi_request_v01_wrapped
from openai import OpenAI, AzureOpenAI
from transformers import AutoTokenizer

try:
    from anthropic import RateLimitError
except ImportError:
    RateLimitError = None

############################
# Code copied from https://github.com/stanfordnlp/dspy/blob/main/dspy/clients/lm.py on Sep 29, 2024

# try:
import warnings

with warnings.catch_warnings():
    warnings.filterwarnings("ignore", category=UserWarning)
    if "LITELLM_LOCAL_MODEL_COST_MAP" not in os.environ:
        os.environ["LITELLM_LOCAL_MODEL_COST_MAP"] = "True"
    import litellm

    litellm.drop_params = True
    litellm.telemetry = False

from litellm.caching.caching import Cache

disk_cache_dir = os.path.join(Path.home(), ".storm_local_cache")
litellm.cache = Cache(disk_cache_dir=disk_cache_dir, type="disk")

# except ImportError:

#     class LitellmPlaceholder:
#         def __getattr__(self, _):
#             raise ImportError(
#                 "The LiteLLM package is not installed. Run `pip install litellm`."
#             )

# litellm = LitellmPlaceholder()
LM_LRU_CACHE_MAX_SIZE = 3000


class LM:
    /* ... body removed by PasteMax ... */s"],
#                 )

#                 if "<|endoftext|>" in tokens:
#                     index = tokens.index("<|endoftext|>") + 1
#                     tokens, logprobs = tokens[:index], logprobs[:index]

#                 avglog = sum(logprobs) / len(logprobs)
#                 scored_completions.append((avglog, self._get_choice_text(c)))

#             scored_completions = sorted(scored_completions, reverse=True)
#             completions = [c for _, c in scored_completions]

#         return completions


# class DeepSeekModel(dspy.OpenAI):
#     """A wrapper class for DeepSeek API, compatible with dspy.OpenAI."""

#     def __init__(
#         self,
#         model: str = "deepseek-chat",
#         api_key: Optional[str] = None,
#         api_base: str = "https://api.deepseek.com",
#         **kwargs,
#     ):
#         super().__init__(model=model, api_key=api_key, api_base=api_base, **kwargs)
#         self._token_usage_lock = threading.Lock()
#         self.prompt_tokens = 0
#         self.completion_tokens = 0
#         self.model = model
#         self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
#         self.api_base = api_base
#         if not self.api_key:
#             raise ValueError(
#                 "DeepSeek API key must be provided either as an argument or as an environment variable DEEPSEEK_API_KEY"
#             )

#     def log_usage(self, response):
#         """Log the total tokens from the DeepSeek API response."""
#         usage_data = response.get("usage")
#         if usage_data:
#             with self._token_usage_lock:
#                 self.prompt_tokens += usage_data.get("prompt_tokens", 0)
#                 self.completion_tokens += usage_data.get("completion_tokens", 0)

#     def get_usage_and_reset(self):
#         """Get the total tokens used and reset the token usage."""
#         usage = {
#             self.model: {
#                 "prompt_tokens": self.prompt_tokens,
#                 "completion_tokens": self.completion_tokens,
#             }
#         }
#         self.prompt_tokens = 0
#         self.completion_tokens = 0
#         return usage

#     # @backoff.on_exception(
#     #     backoff.expo,
#     #     ERRORS,
#     #     max_time=1000,
#     #     on_backoff=backoff_hdlr,
#     #     giveup=giveup_hdlr,
#     # )
#     def _create_completion(self, prompt: str, **kwargs):
#         """Create a completion using the DeepSeek API."""
#         headers = {
#             "Content-Type": "application/json",
#             "Authorization": f"Bearer {self.api_key}",
#         }
#         data = {
#             "model": self.model,
#             "messages": [{"role": "user", "content": prompt}],
#             **kwargs,
#         }
#         response = requests.post(
#             f"{self.api_base}/v1/chat/completions", headers=headers, json=data
#         )
#         response.raise_for_status()
#         return response.json()

#     def __call__(
#         self,
#         prompt: str,
#         only_completed: bool = True,
#         return_sorted: bool = False,
#         **kwargs,
#     ) -> list[dict[str, Any]]:
#         """Call the DeepSeek API to generate completions."""
#         assert only_completed, "for now"
#         assert return_sorted is False, "for now"

#         response = self._create_completion(prompt, **kwargs)

#         # Log the token usage from the DeepSeek API response.
#         self.log_usage(response)

#         choices = response["choices"]
#         completions = [choice["message"]["content"] for choice in choices]

#         history = {
#             "prompt": prompt,
#             "response": response,
#             "kwargs": kwargs,
#         }
#         self.history.append(history)

#         return completions


class AzureOpenAIModel(dspy.LM):
    /* ... body removed by PasteMax ... */#             message.pop("name", None)

#         response = requests.post(
#             f"{self.api_base}/chat/completions", headers=headers, json=data
#         )
#         response.raise_for_status()
#         return response.json()

#     def __call__(
#         self,
#         prompt: str,
#         only_completed: bool = True,
#         return_sorted: bool = False,
#         **kwargs,
#     ) -> list[dict[str, Any]]:
#         """Call the Groq API to generate completions."""
#         assert only_completed, "for now"
#         assert return_sorted is False, "for now"

#         response = self._create_completion(prompt, **kwargs)

#         # Log the token usage from the Groq API response.
#         self.log_usage(response)

#         choices = response["choices"]
#         completions = [choice["message"]["content"] for choice in choices]

#         history = {
#             "prompt": prompt,
#             "response": response,
#             "kwargs": kwargs,
#         }
#         self.history.append(history)

#         return completions


# class ClaudeModel(dspy.dsp.modules.lm.LM):
#     """Copied from dspy/dsp/modules/anthropic.py with the addition of tracking token usage."""

#     def __init__(
#         self,
#         model: str,
#         api_key: Optional[str] = None,
#         api_base: Optional[str] = None,
#         **kwargs,
#     ):
#         super().__init__(model)
#         try:
#             from anthropic import Anthropic
#         except ImportError as err:
#             raise ImportError("Claude requires `pip install anthropic`.") from err

#         self.provider = "anthropic"
#         self.api_key = api_key = (
#             os.environ.get("ANTHROPIC_API_KEY") if api_key is None else api_key
#         )
#         self.api_base = (
#             "https://api.anthropic.com/v1/messages" if api_base is None else api_base
#         )
#         self.kwargs = {
#             "temperature": kwargs.get("temperature", 0.0),
#             "max_tokens": min(kwargs.get("max_tokens", 4096), 4096),
#             "top_p": kwargs.get("top_p", 1.0),
#             "top_k": kwargs.get("top_k", 1),
#             "n": kwargs.pop("n", kwargs.pop("num_generations", 1)),
#             **kwargs,
#             "model": model,
#         }
#         self.history: list[dict[str, Any]] = []
#         self.client = Anthropic(api_key=api_key)
#         self.model = model

#         self._token_usage_lock = threading.Lock()
#         self.prompt_tokens = 0
#         self.completion_tokens = 0

#     def log_usage(self, response):
#         """Log the total tokens from the Anthropic API response."""
#         usage_data = response.usage
#         if usage_data:
#             with self._token_usage_lock:
#                 self.prompt_tokens += usage_data.input_tokens
#                 self.completion_tokens += usage_data.output_tokens

#     def get_usage_and_reset(self):
#         """Get the total tokens used and reset the token usage."""
#         usage = {
#             self.model: {
#                 "prompt_tokens": self.prompt_tokens,
#                 "completion_tokens": self.completion_tokens,
#             }
#         }
#         self.prompt_tokens = 0
#         self.completion_tokens = 0

#         return usage

#     def basic_request(self, prompt: str, **kwargs):
#         raw_kwargs = kwargs
#         kwargs = {**self.kwargs, **kwargs}
#         # caching mechanism requires hashable kwargs
#         kwargs["messages"] = [{"role": "user", "content": prompt}]
#         kwargs.pop("n")
#         response = self.client.messages.create(**kwargs)
#         # history = {
#         #     "prompt": prompt,
#         #     "response": response,
#         #     "kwargs": kwargs,
#         #     "raw_kwargs": raw_kwargs,
#         # }
#         json_serializable_history = {
#             "prompt": prompt,
#             "response": {
#                 "content": response.content[0].text,
#                 "model": response.model,
#                 "role": response.role,
#                 "stop_reason": response.stop_reason,
#                 "stop_sequence": response.stop_sequence,
#                 "type": response.type,
#                 "usage": {
#                     "input_tokens": response.usage.input_tokens,
#                     "output_tokens": response.usage.output_tokens,
#                 },
#             },
#             "kwargs": kwargs,
#             "raw_kwargs": raw_kwargs,
#         }
#         self.history.append(json_serializable_history)
#         return response

#     # @backoff.on_exception(
#     #     backoff.expo,
#     #     (RateLimitError,),
#     #     max_time=1000,
#     #     max_tries=8,
#     #     on_backoff=backoff_hdlr,
#     #     giveup=giveup_hdlr,
#     # )
#     def request(self, prompt: str, **kwargs):
#         """Handles retrieval of completions from Anthropic whilst handling API errors."""
#         return self.basic_request(prompt, **kwargs)

#     def __call__(self, prompt, only_completed=True, return_sorted=False, **kwargs):
#         """Retrieves completions from Anthropic.

#         Args:
#             prompt (str): prompt to send to Anthropic
#             only_completed (bool, optional): return only completed responses and ignores completion due to length. Defaults to True.
#             return_sorted (bool, optional): sort the completion choices using the returned probabilities. Defaults to False.

#         Returns:
#             list[str]: list of completion choices
#         """
#         assert only_completed, "for now"
#         assert return_sorted is False, "for now"
#         # per eg here: https://docs.anthropic.com/claude/reference/messages-examples
#         # max tokens can be used as a proxy to return smaller responses
#         # so this cannot be a proper indicator for incomplete response unless it isnt the user-intent.
#         n = kwargs.pop("n", 1)
#         completions = []
#         for _ in range(n):
#             response = self.request(prompt, **kwargs)
#             self.log_usage(response)
#             # This is the original behavior in dspy/dsp/modules/anthropic.py.
#             # Comment it out because it can cause "IndexError: list index out of range" silently
#             # which is not transparent to developers.
#             # if only_completed and response.stop_reason == "max_tokens":
#             #     continue
#             completions = [c.text for c in response.content]
#         return completions


class VLLMClient(dspy.LM):
    /* ... body removed by PasteMax ... */      f"{self.url}:{random.Random().choice(self.ports)}" + "/generate",
#             url=self.url,
#             ports=tuple(self.ports),
#             json=payload,
#             headers=self.headers,
#             **self.http_request_kwargs,
#         )

#         try:
#             json_response = response.json()
#             # completions = json_response["generated_text"]

#             completions = [json_response["generated_text"]]

#             if (
#                 "details" in json_response
#                 and "best_of_sequences" in json_response["details"]
#             ):
#                 completions += [
#                     x["generated_text"]
#                     for x in json_response["details"]["best_of_sequences"]
#                 ]

#             response = {"prompt": prompt, "choices": [{"text": c} for c in completions]}
#             return response
#         except Exception:
#             print("Failed to parse JSON response:", response.text)
#             raise Exception("Received invalid JSON response from server")


# class TogetherClient(dspy.HFModel):
#     """A wrapper class for dspy.Together."""

#     def __init__(
#         self,
#         model,
#         api_key: Optional[str] = None,
#         apply_tokenizer_chat_template=False,
#         hf_tokenizer_name=None,
#         model_type: Literal["chat", "text"] = "chat",
#         **kwargs,
#     ):
#         """Copied from dspy/dsp/modules/hf_client.py with the support of applying tokenizer chat template."""

#         super().__init__(model=model, is_client=True)
#         self.session = requests.Session()
#         self.api_key = api_key = (
#             os.environ.get("TOGETHER_API_KEY") if api_key is None else api_key
#         )
#         self.model = model
#         self.model_type = model_type
#         if os.getenv("TOGETHER_API_BASE") is None:
#             if self.model_type == "chat":
#                 self.api_base = "https://api.together.xyz/v1/chat/completions"
#             else:
#                 self.api_base = "https://api.together.xyz/v1/completions"
#         else:
#             self.api_base = os.getenv("TOGETHER_API_BASE")

#         # self.use_inst_template = False
#         # if any(keyword in self.model.lower() for keyword in ["inst", "instruct"]):
#         #     self.use_inst_template = True
#         self.apply_tokenizer_chat_template = apply_tokenizer_chat_template
#         if self.apply_tokenizer_chat_template:
#             logging.info("Loading huggingface tokenizer.")
#             if hf_tokenizer_name is None:
#                 hf_tokenizer_name = self.model
#             self.tokenizer = AutoTokenizer.from_pretrained(
#                 hf_tokenizer_name, cache_dir=kwargs.get("cache_dir", None)
#             )

#         stop_default = "\n\n---"

#         self.kwargs = {
#             "temperature": kwargs.get("temperature", 0.0),
#             "max_tokens": min(kwargs.get("max_tokens", 4096), 4096),
#             "top_p": kwargs.get("top_p", 1.0),
#             "top_k": kwargs.get("top_k", 1),
#             "repetition_penalty": 1,
#             "n": kwargs.pop("n", kwargs.pop("num_generations", 1)),
#             "stop": stop_default if "stop" not in kwargs else kwargs["stop"],
#             **kwargs,
#         }
#         self._token_usage_lock = threading.Lock()
#         self.prompt_tokens = 0
#         self.completion_tokens = 0

#     def log_usage(self, response):
#         """Log the total tokens from the OpenAI API response."""
#         usage_data = response.get("usage")
#         if usage_data:
#             with self._token_usage_lock:
#                 self.prompt_tokens += usage_data.get("prompt_tokens", 0)
#                 self.completion_tokens += usage_data.get("completion_tokens", 0)

#     def get_usage_and_reset(self):
#         """Get the total tokens used and reset the token usage."""
#         usage = {
#             self.model: {
#                 "prompt_tokens": self.prompt_tokens,
#                 "completion_tokens": self.completion_tokens,
#             }
#         }
#         self.prompt_tokens = 0
#         self.completion_tokens = 0

#         return usage

#     # @backoff.on_exception(
#     #     backoff.expo,
#     #     ERRORS,
#     #     max_time=1000,
#     #     on_backoff=backoff_hdlr,
#     # )
#     def _generate(self, prompt, **kwargs):
#         kwargs = {**self.kwargs, **kwargs}

#         stop = kwargs.get("stop")
#         temperature = kwargs.get("temperature")
#         max_tokens = kwargs.get("max_tokens", 150)
#         top_p = kwargs.get("top_p", 0.7)
#         top_k = kwargs.get("top_k", 50)
#         repetition_penalty = kwargs.get("repetition_penalty", 1)
#         if self.apply_tokenizer_chat_template:
#             prompt = self.tokenizer.apply_chat_template(
#                 [{"role": "user", "content": prompt}], tokenize=False
#             )
#         # prompt = f"[INST]{prompt}[/INST]" if self.use_inst_template else prompt

#         if self.model_type == "chat":
#             messages = [
#                 {
#                     "role": "system",
#                     "content": "You are a helpful assistant. You must continue the user text directly without *any* additional interjections.",
#                 },
#                 {"role": "user", "content": prompt},
#             ]
#             body = {
#                 "model": self.model,
#                 "messages": messages,
#                 "temperature": temperature,
#                 "max_tokens": max_tokens,
#                 "top_p": top_p,
#                 "top_k": top_k,
#                 "repetition_penalty": repetition_penalty,
#                 "stop": stop,
#             }
#         else:
#             body = {
#                 "model": self.model,
#                 "prompt": prompt,
#                 "temperature": temperature,
#                 "max_tokens": max_tokens,
#                 "top_p": top_p,
#                 "top_k": top_k,
#                 "repetition_penalty": repetition_penalty,
#                 "stop": stop,
#             }

#         headers = {"Authorization": f"Bearer {self.api_key}"}

#         with self.session.post(self.api_base, headers=headers, json=body) as resp:
#             resp_json = resp.json()
#             # Log the token usage from the Together API response.
#             self.log_usage(resp_json)
#             if self.model_type == "chat":
#                 # completions = [resp_json['output'].get('choices', [])[0].get('message', {}).get('content', "")]
#                 completions = [
#                     resp_json.get("choices", [])[0]
#                     .get("message", {})
#                     .get("content", "")
#                 ]
#             else:
#                 # completions = [resp_json['output'].get('choices', [])[0].get('text', "")]
#                 completions = [resp_json.get("choices", [])[0].get("text", "")]
#             response = {"prompt": prompt, "choices": [{"text": c} for c in completions]}
#             return response


# class GoogleModel(dspy.dsp.modules.lm.LM):
#     """A wrapper class for Google Gemini API."""

#     def __init__(
#         self,
#         model: str,
#         api_key: Optional[str] = None,
#         **kwargs,
#     ):
#         """You can use `genai.list_models()` to get a list of available models."""
#         super().__init__(model)
#         try:
#             import google.generativeai as genai
#         except ImportError as err:
#             raise ImportError(
#                 "GoogleModel requires `pip install google-generativeai`."
#             ) from err

#         api_key = os.environ.get("GOOGLE_API_KEY") if api_key is None else api_key
#         genai.configure(api_key=api_key)

#         kwargs = {
#             "candidate_count": 1,  # Caveat: Gemini API supports only one candidate for now.
#             "temperature": (
#                 0.0 if "temperature" not in kwargs else kwargs["temperature"]
#             ),
#             "max_output_tokens": kwargs["max_tokens"],
#             "top_p": 1,
#             "top_k": 1,
#             **kwargs,
#         }

#         kwargs.pop("max_tokens", None)  # GenerationConfig cannot accept max_tokens

#         self.model = model
#         self.config = genai.GenerationConfig(**kwargs)
#         self.llm = genai.GenerativeModel(
#             model_name=model, generation_config=self.config
#         )

#         self.kwargs = {
#             "n": 1,
#             **kwargs,
#         }

#         self.history: list[dict[str, Any]] = []

#         self._token_usage_lock = threading.Lock()
#         self.prompt_tokens = 0
#         self.completion_tokens = 0

#     def log_usage(self, response):
#         """Log the total tokens from the Google API response."""
#         usage_data = response.usage_metadata
#         if usage_data:
#             with self._token_usage_lock:
#                 self.prompt_tokens += usage_data.prompt_token_count
#                 self.completion_tokens += usage_data.candidates_token_count

#     def get_usage_and_reset(self):
#         """Get the total tokens used and reset the token usage."""
#         usage = {
#             self.model: {
#                 "prompt_tokens": self.prompt_tokens,
#                 "completion_tokens": self.completion_tokens,
#             }
#         }
#         self.prompt_tokens = 0
#         self.completion_tokens = 0

#         return usage

#     def basic_request(self, prompt: str, **kwargs):
#         raw_kwargs = kwargs
#         kwargs = {
#             **self.kwargs,
#             **kwargs,
#         }

#         # Google disallows "n" arguments.
#         n = kwargs.pop("n", None)

#         response = self.llm.generate_content(prompt, generation_config=kwargs)

#         history = {
#             "prompt": prompt,
#             "response": [response.to_dict()],
#             "kwargs": kwargs,
#             "raw_kwargs": raw_kwargs,
#         }
#         self.history.append(history)

#         return response

#     # @backoff.on_exception(
#     #     backoff.expo,
#     #     (Exception,),
#     #     max_time=1000,
#     #     max_tries=8,
#     #     on_backoff=backoff_hdlr,
#     #     giveup=giveup_hdlr,
#     # )
#     def request(self, prompt: str, **kwargs):
#         """Handles retrieval of completions from Google whilst handling API errors"""
#         return self.basic_request(prompt, **kwargs)

#     def __call__(
#         self,
#         prompt: str,
#         only_completed: bool = True,
#         return_sorted: bool = False,
#         **kwargs,
#     ):
#         assert only_completed, "for now"
#         assert return_sorted is False, "for now"

#         n = kwargs.pop("n", 1)

#         completions = []
#         for _ in range(n):
#             response = self.request(prompt, **kwargs)
#             self.log_usage(response)
#             completions.append(response.parts[0].text)

#         return completions


# ========================================================================

```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/logging_wrapper.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/main.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/utils/math_equivalence.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/mistral_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/models.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/utils/multiprocess.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/oai_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/old_results_check.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/outline_generation.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/parser.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/pass_k_utils.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/utils/path_utils.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/persona_generator.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/pyext/pyext-0.7/PKG-INFO
```7/PKG-INFO
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/prompt_manager.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/prompts.py
```py
def get_hard_question_instruction(MAX_SEARCH_LIMIT):
    /* ... body removed by PasteMax ... */


def get_math_search_o1_instruction(MAX_SEARCH_LIMIT):
    /* ... body removed by PasteMax ... */


def get_code_search_o1_instruction(MAX_SEARCH_LIMIT):
    /* ... body removed by PasteMax ... */


def get_webpage_to_reasonchain_instruction(prev_reasoning, search_query, document):
    /* ... body removed by PasteMax ... */


def get_singleqa_search_o1_instruction(MAX_SEARCH_LIMIT):
    /* ... body removed by PasteMax ... */

def get_multiqa_search_o1_instruction(MAX_SEARCH_LIMIT):
    /* ... body removed by PasteMax ... */

    
def get_singleqa_rag_agent_instruction(MAX_SEARCH_LIMIT, MAX_URL_FETCH):
    /* ... body removed by PasteMax ... */


def get_multiqa_rag_agent_instruction(MAX_SEARCH_LIMIT, MAX_URL_FETCH):
    /* ... body removed by PasteMax ... */


def get_gpqa_rag_agent_instruction(MAX_SEARCH_LIMIT, MAX_URL_FETCH):
    /* ... body removed by PasteMax ... */


def get_math_rag_agent_instruction(MAX_SEARCH_LIMIT, MAX_URL_FETCH):
    /* ... body removed by PasteMax ... */


def get_code_rag_agent_instruction(MAX_SEARCH_LIMIT, MAX_URL_FETCH):
    /* ... body removed by PasteMax ... */


def get_naive_rag_instruction(question, documents):
    /* ... body removed by PasteMax ... */



def get_task_instruction_openqa(question, model_name=None):
    /* ... body removed by PasteMax ... */

def get_task_instruction_math(question, model_name=None):
    /* ... body removed by PasteMax ... */

def get_task_instruction_multi_choice(question, model_name=None):
    /* ... body removed by PasteMax ... */

def get_task_instruction_code(question, question_title=None, model_name=None):
    /* ... body removed by PasteMax ... */


```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/pyext/pyext-0.7/pyext.py
```py
'''
Copyright (C) 2014 Ryan Gonzalez


Permission is hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the "Software"), to deal in
the Software without restriction, including without limitation the rights to use,
copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the
Software, and to permit persons to whom the Software is furnished to do so,
subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
'''

g_backup = globals().copy()

__version__ = '0.7'

__all__ = ['overload', 'RuntimeModule', 'switch', 'tail_recurse', 'copyfunc', 'set_docstring', 'annotate', 'safe_unpack', 'modify_function', 'assign', 'fannotate', 'compare_and_swap', 'is_main', 'call_if_main', 'run_main']

import sys, inspect, types

def __targspec(func, specs, attr='__orig_arg__'):
    /* ... body removed by PasteMax ... */

def set_docstring(doc):
    /* ... body removed by PasteMax ... */ _wrap

__modify_function_doc = '''
Creates a copy of a function, changing its attributes.

:param globals: Will be added to the function's globals.

:param name: The new function name. Set to ``None`` to use the function's original name.

:param code: The new function code object. Set to ``None`` to use the function's original code object.

:param defaults: The new function defaults. Set to ``None`` to use the function's original defaults.

:param closure: The new function closure. Set to ``None`` to use the function's original closure.

.. warning:: This function can be potentially dangerous.
'''

def copyfunc(f):
   /* ... body removed by PasteMax ... */

if sys.version_info.major == 3:
    @set_docstring(__modify_function_doc)
    def modify_function(f, globals={}, name=None, code=None, defaults=None,
                        closure=None):
        /* ... body removed by PasteMax ... */
    def argspec(f):
        /* ... body removed by PasteMax ... */
    ofullargspec = inspect.getfullargspec
    def _fullargspec(func):
        /* ... body removed by PasteMax ... */
    inspect.getfullargspec = _fullargspec
    def _exec(m,g): /* ... body removed by PasteMax ... */
else:
    @set_docstring(__modify_function_doc)
    def modify_function(f, globals={}, name=None, code=None, defaults=None,
                        closure=None):
        /* ... body removed by PasteMax ... */
    def argspec(f):
        /* ... body removed by PasteMax ... */
    eval(compile('def _exec(m,g): exec m in g', '<exec>', 'exec'))

def _gettypes(args):
    /* ... body removed by PasteMax ... */

# oargspec = inspect.getargspec
oargspec = inspect.getfullargspec

def _argspec(func):
    /* ... body removed by PasteMax ... */

inspect.getargspec = _argspec

try:
    import IPython
except ImportError:
    IPython = None
else:
    # Replace IPython's argspec
    oipyargspec = IPython.core.oinspect.getargspec
    def _ipyargspec(func):
        /* ... body removed by PasteMax ... */
    IPython.core.oinspect.getargspec = _ipyargspec

class overload(object):
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/pyproject.toml
```toml
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/README.md
```md
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/pyext/pyext-0.7/README.rst
```rst
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/utils/remote_llm.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/retriever.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/rm.py
```py
import logging
import os
from typing import Callable, Union, List

import backoff
import dspy
import requests
# from dsp import backoff_hdlr, giveup_hdlr

from .utils import WebPageHelper


class YouRM(dspy.Retrieve):
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_research/rm.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/run_agentic_reason.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/run_code.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/run_search.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/runner_utils.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/scenario_router.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/utils/scenarios.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/search.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/prompts/self_repair.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/pyext/pyext-0.7/setup.cfg
```cfg
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/pyext/pyext-0.7/setup.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/setup.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/simulate_user.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/prompts/few_shot_examples/generation/stdin.json
```json
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/storm_analysis/modules/storm_dataclass.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/tools/temp.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/temp.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/testing_util.py
```py
import ast
import json
import sys
import faulthandler
import platform

# used for debugging to time steps
from datetime import datetime

# to run the solution files we're using a timing based approach
import signal

import numpy as np

# for capturing the stdout
from io import StringIO

# used for testing the code that reads from input
from unittest.mock import patch, mock_open

# from pyext import RuntimeModule

from enum import Enum


def truncatefn(s, length=300):
    /* ... body removed by PasteMax ... */


class CODE_TYPE(Enum):
    /* ... body removed by PasteMax ... */


# stuff for setting up signal timer
class TimeoutException(Exception):
    /* ... body removed by PasteMax ... */


def timeout_handler(signum, frame):
    /* ... body removed by PasteMax ... */


signal.signal(signal.SIGALRM, timeout_handler)
# timeout = 6  # seconds


# used to capture stdout as a list
# from https://stackoverflow.com/a/16571630/6416660
# alternative use redirect_stdout() from contextlib
class Capturing(list):
    /* ... body removed by PasteMax ... */test(sample, test=None, debug=False, timeout=6):
    /* ... body removed by PasteMax ... */


def custom_compare_(output, ground_truth):

    /* ... body removed by PasteMax ... */


def stripped_string_compare(s1, s2):
    /* ... body removed by PasteMax ... */


def call_method(method, inputs):

    /* ... body removed by PasteMax ... */y PasteMax ... */


# Replace with a simple implementation
class RuntimeModule:
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/evaluation/utils_execute.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/utils.py
```py
import concurrent.futures
import dspy
import httpx
import json
import logging
import os
import pickle
import re
import regex
import sys
import toml
from typing import List, Dict
from tqdm import tqdm

from langchain_text_splitters import RecursiveCharacterTextSplitter
from trafilatura import extract

from .lm import LitellmModel

logging.getLogger("httpx").setLevel(logging.WARNING)  # Disable INFO logging for httpx.


def truncate_filename(filename, max_length=125):
    /* ... body removed by PasteMax ... */


def load_api_key(toml_file_path):
    /* ... body removed by PasteMax ... */


def makeStringRed(message):
    /* ... body removed by PasteMax ... */


class QdrantVectorStoreManager:
    /* ... body removed by PasteMax ... */
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/agentic_reason/utils.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/lcb_runner/runner/vllm_runner.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/agentic_research/collaborative_storm/modules/warmstart_hierarchical_chat.py
```py
// Content unavailable
```

### /Users/jesper/Projects/Other_projects/Installs/Agentic-Reasoning/scripts/write_paper.py
```py
// Content unavailable
```

